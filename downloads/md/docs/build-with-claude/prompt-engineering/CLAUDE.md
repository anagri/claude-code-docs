# Directory: prompt-engineering

## Overview
This directory contains comprehensive guides on prompt engineering techniques for optimizing Claude's performance across various tasks and use cases. These resources cover fundamental best practices, advanced techniques, and model-specific optimizations.

## Files in This Directory

### **be-clear-and-direct.md**
This guide emphasizes the importance of explicit, contextual, and specific instructions when prompting Claude. The "golden rule" is to ensure prompts are clear enough that a colleague with minimal context could follow them. Key principles include: providing contextual information (task purpose, audience, workflow position, success criteria), being specific about desired outputs, and structuring instructions as sequential steps. The guide demonstrates through examples how vague prompts lead to incomplete or fabricated results, while clear prompts with detailed instructions produce accurate, well-structured outputs. Examples cover anonymizing customer feedback, crafting marketing emails, and incident response analysis.

### **chain-of-thought.md**
Chain of thought (CoT) prompting encourages Claude to break down complex problems step-by-step, dramatically improving accuracy, coherence, and debuggability for tasks requiring research, analysis, or problem-solving. The guide presents three levels of CoT complexity: basic ("Think step-by-step"), guided (outlining specific thinking steps), and structured (using XML tags like `<thinking>` and `<answer>` to separate reasoning from results). The technique is particularly valuable for math, logic, complex analysis, and multi-step tasks, though it increases output length and may impact latency. Examples demonstrate how CoT transforms superficial recommendations into deeply analytical, quantified insights with thorough risk assessment.

### **chain-prompts.md**
Prompt chaining breaks complex tasks into smaller, manageable subtasks where each step gets Claude's full attention, improving accuracy, clarity, and traceability. This technique is essential for multi-step workflows like research synthesis, document analysis, iterative content creation, and verification loops. The guide outlines how to structure chains with XML tags for clear handoffs, maintain single-task goals per step, and run independent subtasks in parallel for speed. Advanced applications include self-correction chains where Claude reviews and refines its own work. Detailed examples show legal contract analysis and multitenancy strategy reviews, demonstrating how chaining transforms shallow initial responses into comprehensive, actionable documents with proper analysis and recommendations.

### **claude-4-best-practices.md**
This comprehensive guide provides specific techniques for Claude 4 models (Opus 4.1, Opus 4, Sonnet 4.5, Sonnet 4), which are trained for more precise instruction following. Key principles include: being explicit with instructions and adding context/motivation to help Claude understand goals better. Claude Sonnet 4.5 excels at long-horizon reasoning with exceptional state tracking across multiple context windows. The model features context awareness (tracking remaining token budget) and works effectively with tools like the memory tool for seamless context transitions. Best practices cover multi-context window workflows (using structured formats for state data, git for tracking, and encouraging complete context usage), communication style (more concise and direct), tool usage patterns (requiring explicit direction), controlling output formatting, research capabilities, subagent orchestration, document creation, parallel tool calling, visual/frontend code generation, and strategies to minimize hallucinations in agentic coding.

### **extended-thinking-tips.md**
This advanced guide provides strategies for maximizing Claude's extended thinking capabilities, which allow step-by-step problem solving for complex tasks. Technical considerations include starting with the minimum 1024-token thinking budget and incrementally increasing, using batch processing for budgets above 32K tokens, and ensuring prompts are in English (though outputs can be in any language). Key techniques include: using general instructions first rather than prescriptive step-by-step guidance (allowing Claude's creativity to shine), leveraging multishot prompting with `<thinking>` tags for canonical patterns, maximizing instruction following by allowing sufficient budget for reasoning, and using Claude's thinking output to debug logic. The guide covers optimization for long outputs (requesting detailed outlines with word counts, indexing paragraphs), with examples for complex STEM problems, constraint optimization, and thinking frameworks. It also demonstrates how to have Claude reflect on and verify its work to improve consistency and error handling.

### **long-context-tips.md**
This guide optimizes use of Claude's 200K token context window for handling complex, data-rich tasks. Essential tips include: placing longform data (20K+ tokens) at the top of prompts above queries and instructions (improving response quality by up to 30%), structuring document content and metadata with XML tags like `<document>`, `<source>`, and `<document_content>`, and grounding responses in quotes by asking Claude to extract relevant passages first before completing tasks. This quote-extraction technique helps Claude "cut through the noise" of large documents. The structured approach is particularly effective for multi-document analysis, medical diagnosis support, and other scenarios requiring precise information retrieval from extensive content.

### **multishot-prompting.md**
Examples are the "secret weapon" for getting Claude to generate exactly what you need through few-shot or multishot prompting. Providing 3-5 diverse, relevant examples dramatically improves accuracy, consistency, and performance, especially for tasks requiring structured outputs or specific formats. Effective examples should be: relevant (mirroring actual use cases), diverse (covering edge cases without unintended patterns), and clear (wrapped in `<example>` tags, nested within `<examples>` for multiple). The technique is particularly powerful for enforcing uniform structure and style while reducing misinterpretation. The guide demonstrates customer feedback analysis where examples ensure consistent categorization, proper sentiment/priority rating, and appropriate output formatting that unclear prompts fail to achieve.

### **overview.md**
This overview establishes the foundation for prompt engineering, emphasizing prerequisites: clear success criteria, empirical testing methods, and a first draft prompt. It advocates prompt engineering over finetuning due to superior resource efficiency, cost-effectiveness, maintaining model updates, time-saving, minimal data needs, flexibility for rapid iteration, domain adaptation, comprehension improvements, preserving general knowledge, and transparency. The guide presents a recommended sequence of techniques ordered from most broadly effective to specialized: prompt generator, be clear and direct, use examples (multishot), let Claude think (chain of thought), use XML tags, give Claude a role (system prompts), prefill Claude's response, chain complex prompts, and long context tips. Links to interactive tutorials (GitHub and Google Sheets versions) are provided for hands-on learning.

### **prefill-claudes-response.md**
Prefilling (only available for non-extended thinking modes) allows guiding Claude's responses by providing initial text in the Assistant message, which Claude continues from. This powerful technique directs actions, skips preambles, enforces specific formats (JSON/XML), and maintains character consistency in role-play scenarios. The implementation involves including desired initial text in the Assistant message without trailing whitespace. Key use cases include: controlling output formatting (prefilling `{` forces immediate JSON output without preamble, making responses cleaner and easier to parse programmatically), and maintaining character in roleplay (prefilling `[ROLE_NAME]` keeps Claude in character across long conversations, especially when combined with system prompts). Examples demonstrate how prefilling transforms verbose, explanation-heavy responses into concise, properly formatted outputs.

### **prompt-generator.md**
The prompt generator tool in the Claude Console helps solve the "blank page problem" by guiding Claude to generate high-quality prompt templates tailored to specific tasks. These templates follow prompt engineering best practices and work with all Claude models, including extended thinking variants. The tool is accessible directly through the Console, with an underlying architecture detailed in a Google Colab notebook for those interested in the meta-prompt approach. The notebook allows users to run code and have Claude construct prompts programmatically, requiring only an API key. This tool serves as an excellent starting point for further testing and iteration, particularly for users new to prompt engineering or facing complex new use cases.

### **prompt-improver.md**
The prompt improver helps quickly iterate and improve prompts through automated analysis and enhancement, excelling at making prompts more robust for complex, high-accuracy tasks. It works in four steps: example identification (extracting existing examples), initial draft (creating structured templates with XML tags), chain of thought refinement (adding detailed reasoning instructions), and example enhancement (updating examples to demonstrate new reasoning). The tool generates templates with detailed CoT instructions, clear XML organization, standardized example formatting showing step-by-step reasoning, and strategic prefills. Users should provide: a prompt template, feedback on current issues, and example inputs/ideal outputs (optional but recommended). The tool works best for complex tasks requiring detailed reasoning where accuracy matters more than speed, though it produces longer, more thorough but slower responses. An integrated Test Case Generator helps create examples if none exist.

### **prompt-templates-and-variables.md**
Prompt templates combine fixed content (static instructions/context) with variable content (dynamic elements like user inputs, RAG-retrieved content, conversation context, or tool use results). In the Claude Console, variables are denoted with double brackets `{{VARIABLE_NAME}}`, enabling quick testing of different values. Templates offer significant benefits: consistency in prompt structure, efficiency in swapping variable content, testability for different inputs and edge cases, scalability as applications grow, and version control for tracking changes separate from dynamic inputs. The Console's prompt generator, prompt improver, and evaluation tool all leverage templates to support rapid iteration and testing. A simple example shows a translation app using `{{text}}` as a variable for content that changes between users or API calls. Templates are essential whenever any part of a prompt will be repeated across multiple Claude interactions.

### **system-prompts.md**
Role prompting through the `system` parameter is the most powerful way to use system prompts with Claude, transforming it from a general assistant into a virtual domain expert. Benefits include enhanced accuracy (particularly for complex scenarios like legal analysis or financial modeling), tailored tone (adjusting communication style from CFO brevity to copywriter flair), and improved focus (keeping Claude within task-specific requirements). Implementation uses the `system` parameter in the Messages API to set Claude's role, with all task-specific instructions placed in the `user` turn. Detailed examples demonstrate dramatic performance improvements: a legal contract analysis where adding a General Counsel role helps Claude catch critical issues that could cost millions (versus missing them without a role), and financial analysis where a CFO role produces actionable strategic insights and specific recommendations versus superficial summaries. Experimenting with role specificity (e.g., "data scientist" vs. "data scientist specializing in customer insight analysis for Fortune 500 companies") can yield different results.

### **use-xml-tags.md**
XML tags are game-changers for multi-component prompts involving context, instructions, and examples, helping Claude parse prompts more accurately for higher-quality outputs. Benefits include clarity (separating different parts and ensuring structure), accuracy (reducing misinterpretation errors), flexibility (easily modifying parts without rewriting everything), and parseability (making Claude's output easier to extract via post-processing). Best practices emphasize consistency (using same tag names throughout), nesting tags hierarchically (`<outer><inner></inner></outer>`), and combining with other techniques like multishot prompting (`<examples>`) or chain of thought (`<thinking>`, `<answer>`). While there are no canonical "best" XML tags, tag names should make sense with the content they surround. Examples demonstrate how XML tags transform disorganized financial reports and legal analyses into structured, thorough outputs that teams can act on, with clear separation of data, instructions, and formatting requirements.
