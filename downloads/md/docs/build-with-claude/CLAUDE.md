# Directory: build-with-claude

## Overview
This directory contains comprehensive documentation on building applications with Claude, covering core capabilities, API features, tools, and prompt engineering techniques to optimize Claude's performance across various use cases.

## Files in This Directory

### **batch-processing.md**
The Message Batches API enables asynchronous processing of large volumes of requests at 50% cost reduction with most batches completing in under 1 hour. Batches support up to 100,000 requests or 256 MB per batch and are ideal for large-scale evaluations, content moderation, data analysis, and bulk content generation. Results remain available for 29 days. The API supports all Claude 4 and 3 models with full Messages API features including vision, tool use, system messages, and multi-turn conversations. Implementation involves creating batches with unique custom_ids, polling for completion status, and streaming results in JSONL format. Best practices include testing request shapes with the Messages API first, using meaningful custom_ids for result matching, and monitoring batch status regularly. Prompt caching can be used with batches for cost savings, though cache hits are best-effort due to concurrent processing.

### **citations.md**
The citations feature enables Claude to provide detailed source references when answering questions about documents, available on Claude Opus 4.1/4, Sonnet 4.5/4/3.7, Sonnet 3.5 v2, and Haiku 3.5. Citations work by chunking documents (PDFs into sentences, plain text into sentences, custom content as-is) and returning specific references with cited text and location information. Implementation involves enabling citations with `citations.enabled=true` on document blocks and receiving responses with text blocks containing citation arrays pointing to exact sources. Three document types are supported: plain text (sentence chunking, character indices), PDFs (sentence chunking, page numbers), and custom content (no chunking, block indices). The response structure includes multiple text blocks with citations containing type, cited text, document index/title, and location pointers. Compared to prompt-based approaches, the citations feature offers cost savings (cited_text doesn't count toward output tokens), better reliability (guaranteed valid pointers), and improved quality. The feature works with prompt caching, token counting, and batch processing.

### **context-editing.md**
Context editing (beta with `context-management-2025-06-27` header) automatically manages conversation context by clearing tool results when approaching token thresholds, currently supporting the `clear_tool_uses_20250919` strategy on Claude Opus 4.1/4 and Sonnet 4.5/4. When activated, the API clears oldest tool results chronologically, replacing them with placeholders. Users can optionally clear both results and tool calls with `clear_tool_inputs=true`. Configuration options include trigger thresholds (default 100K input tokens), keep counts (default 3 tool uses), clear_at_least minimums for cache worthiness, excluded tools, and tool input clearing. Context editing invalidates cached prompt prefixes since clearing modifies prompt structure, so the `clear_at_least` parameter ensures clearing enough tokens to justify cache invalidation. The response format includes applied_edits showing cleared tool uses and tokens. The feature works with token counting endpoint for previewing post-editing token usage.

### **context-windows.md**
Claude's context window represents working memory for processing text and generating responses, distinct from training data. The standard 200K token capacity grows linearly with conversation turns, with each turn containing input (all previous history plus current message) and output phases. When using extended thinking, thinking blocks from previous turns are automatically stripped and don't count toward context window, though current turn thinking does count. With tool use and extended thinking, thinking blocks must be preserved during tool use for reasoning continuity. Claude Sonnet 4/4.5 support a 1M token context window (beta for tier 4 organizations) accessed via the `context-1m-2025-08-07` beta header, with premium pricing (2x input, 1.5x output) for requests exceeding 200K tokens. Claude Sonnet 4.5 features context awareness, explicitly tracking remaining token budget throughout conversations to enable better task persistence. Newer Claude models (starting with 3.7) return validation errors rather than silently truncating when prompt + output tokens exceed the context window.

### **define-success.md**
Building successful LLM applications starts with clearly defining success criteria that are specific, measurable, achievable, and relevant. Good criteria use quantitative metrics or well-defined qualitative scales, with even "hazy" topics like ethics quantifiable (e.g., "less than 0.1% of 10,000 trials flagged for toxicity"). Measurement methods include quantitative metrics (F1 score, BLEU, accuracy, precision, recall, response time) and qualitative scales (Likert scales, expert rubrics). Common criteria to consider include task fidelity, consistency, relevance and coherence, tone and style, privacy preservation, context utilization, latency, and price. Most use cases require multidimensional evaluation across several criteria. The guide provides examples contrasting bad criteria ("classify sentiments well") with good ones that specify measurable targets, test sets, and baselines.

### **develop-tests.md**
After defining success criteria, design evaluations to measure performance through task-specific test cases that mirror real-world distributions including edge cases (irrelevant inputs, overly long data, poor user input, ambiguous cases). Evaluation principles emphasize automating when possible (multiple-choice, string match, code-graded, LLM-graded), prioritizing volume over quality (more questions with automated grading beats fewer with manual grading). Example eval types include exact match (sentiment analysis), cosine similarity (FAQ bot consistency), ROUGE-L (summarization relevance), LLM-based Likert scales (customer service tone), binary classification (privacy preservation), and ordinal scales (context utilization). For grading, use code-based methods when possible (fastest, most reliable, scalable), LLM-based grading for complex judgments (with detailed rubrics, empirical outputs, and reasoning), and human grading as a last resort. Tips for LLM grading include detailed clear rubrics, empirical/specific instructions, and encouraging reasoning before scoring.

### **embeddings.md**
Anthropic doesn't offer its own embedding model but recommends Voyage AI, which provides state-of-the-art embeddings with options for general-purpose, domain-specific, and multimodal use cases. Available models include voyage-3-large (best quality), voyage-3.5 (balanced), voyage-3.5-lite (optimized for latency/cost), voyage-code-3 (code retrieval), voyage-finance-2 (finance), voyage-law-2 (legal/long-context), and voyage-multimodal-3 (interleaved text and images). Embeddings can be obtained via the Voyage Python package or HTTP API, with both supporting input_type parameter ("document" or "query") for retrieval tasks. The quickstart demonstrates semantic search by embedding documents and queries, then using cosine similarity for retrieval. Voyage embeddings are normalized to length 1, making dot-product and cosine similarity equivalent. The guide covers quantization options (float, int8, uint8, binary, ubinary) for reducing storage and costs, and Matryoshka embedding truncation for flexible dimension sizes. Voyage is also available on AWS Marketplace.

### **extended-thinking.md**
Extended thinking gives Claude enhanced reasoning capabilities for complex tasks with transparency into step-by-step thought processes before final answers. Supported on Claude Opus 4.1/4, Sonnet 4.5/4/3.7, it requires setting `thinking.type="enabled"` with a `budget_tokens` parameter (minimum 1024, recommended starting point for complex tasks 16K+). Claude 4 models return summarized thinking (billing for full tokens generated, not summary tokens), while Sonnet 3.7 returns full thinking. Streaming is supported via `thinking_delta` events followed by `signature_delta`. When using tools, thinking blocks must be preserved and passed back unmodified for the last assistant turn to maintain reasoning continuity. Claude 4 supports interleaved thinking (beta header `interleaved-thinking-2025-05-14`), enabling reasoning between tool calls with budget_tokens exceeding max_tokens. Thinking blocks from previous turns are stripped and don't count toward context window, though they're cached when using tool results. With prompt caching, thinking parameter changes invalidate message cache but not system/tools cache. Pricing follows standard token rates with thinking charged as output tokens. Best practices include starting with minimum budget and increasing incrementally, using batch processing for budgets above 32K, and monitoring token usage for cost optimization.

### **files.md**
The Files API (beta with `files-api-2025-04-14` header) enables uploading and managing files for use with Claude without re-uploading content with each request, particularly useful with the code execution tool and for frequently-used documents/images. Supports all models that support the given file type (images in Claude 3+, PDFs in Claude 3.5+, various types for code execution in Haiku 3.5+ and Claude 3.7+). Implementation involves uploading files to receive a unique file_id, then referencing that ID in Messages requests. File types map to content blocks: PDFs/text to document blocks, images to image blocks, datasets to container_upload blocks. For unsupported document formats (.csv, .txt, .md, .docx, .xlsx), convert to plain text and include directly in messages. Storage limits are 500 MB per file and 100 GB per organization. Files persist until deletion, are scoped to workspace, and results are available for download only for files created by code execution tool. File operations (upload, download, list, retrieve, delete) are free, while file content in Messages requests is priced as input tokens. Rate limits during beta are ~100 requests per minute.

### **multilingual-support.md**
Claude demonstrates robust multilingual capabilities with strong zero-shot performance across languages. Performance data shows Claude 4 models maintain 95-98% of English performance for major European and Asian languages, with particularly strong results for Spanish (98%), Portuguese (97.3%), Italian (97.5%), and French (97.7%). Lower-resource languages like Bengali (95.2%), Swahili (89.5%), and Yoruba (78.9%) show decreased but still meaningful performance. The metrics are based on human-translated MMLU test sets across 14 languages. Best practices include providing clear language context (though Claude can auto-detect), using native scripts rather than transliteration, and considering cultural context beyond pure translation. Claude processes input and generates output in most Unicode languages, with particularly strong capabilities in widely-spoken languages.

### **overview.md**
This comprehensive reference table lists all Claude's core capabilities and tools. Core capabilities include: 1M token context window (beta), batch processing (50% cost savings), citations, context editing (beta), extended thinking, Files API (beta), PDF support, prompt caching (5m and 1hr), search results, token counting, and tool use. Tools include: Bash, code execution (beta), computer use (beta), fine-grained tool streaming, MCP connector (beta), memory (beta), text editor, web fetch (beta), and web search. Each entry specifies availability across Claude API, Amazon Bedrock, and Google Cloud's Vertex AI platforms. The table serves as a quick reference for feature availability and implementation planning.

### **pdf-support.md**
Claude can analyze text, pictures, charts, and tables in PDFs (up to 100 pages, 32MB per request) on Opus 4.1/4, Sonnet 4.5/4/3.7/3.5, and Haiku 3.5. PDFs can be provided as URLs, base64-encoded data, or via Files API file_id. The system converts each page to an image while extracting text, enabling analysis of both textual and visual content. Token costs include both text (1,500-3,000 tokens per page) and image tokens (calculated via width*height/750 for non-resized images). Amazon Bedrock has two modes: basic text extraction (Converse Document Chat, ~1K tokens for 3 pages) and full visual analysis (Claude PDF Chat, ~7K tokens for 3 pages, requires citations enabled). Best practices include placing PDFs before text in requests, using standard fonts, ensuring clear legible text, rotating pages upright, using logical page numbers, splitting large PDFs, and enabling prompt caching for repeated analysis. The feature integrates with prompt caching, batch processing, and tool use for scalable document processing workflows.

### **prompt-caching.md**
Prompt caching optimizes API usage by caching prompt prefixes with 5-minute default lifetime (refreshed on each use) or 1-hour duration for less frequent access. Using `cache_control: {"type": "ephemeral"}` on content blocks, users can cache tools, system messages, text messages, images/documents, and tool use/results. The system automatically checks for cache hits at previous content block boundaries (up to ~20 blocks) from any explicit breakpoint, so typically only one breakpoint at the end of static content is sufficient. Pricing: 5m cache writes cost 1.25x base input, 1h writes cost 2x base input, cache reads cost 0.1x base input. Minimum cacheable lengths are 1024 tokens (Opus/Sonnet) or 2048 tokens (Haiku). Cache invalidation occurs when modifying cached content, with hierarchical invalidation (tools → system → messages). Changes to tool definitions, web search/citations toggles invalidate entire cache; tool_choice, images, thinking parameters invalidate messages cache only. Best practices include caching stable reusable content at prompt beginning, using strategic breakpoints for different change frequencies, and analyzing cache hit rates. The feature works with batch processing (best-effort cache hits) and extended thinking (thinking blocks can't be cached directly but are cached alongside other content).

### **prompt-engineering.md**
This overview establishes that prompt engineering should begin after defining success criteria, creating empirical tests, and drafting an initial prompt. The guide focuses on success criteria controllable through prompting rather than model selection or finetuning. Prompt engineering is advocated over finetuning due to resource efficiency, cost-effectiveness, maintaining model updates, time-saving, minimal data needs, rapid iteration flexibility, domain adaptation, comprehension improvements, preserving general knowledge, and transparency. The recommended technique sequence from most to least broadly effective is: prompt generator, be clear and direct, use examples (multishot), let Claude think (chain of thought), use XML tags, give Claude a role (system prompts), prefill Claude's response, chain complex prompts, and long context tips. Interactive tutorials are available on GitHub and Google Sheets for hands-on learning.

### **search-results.md**
Search result content blocks enable natural citations with proper source attribution for RAG applications, available on Haiku 3.5, Sonnet 3.5/3.7/4/4.5, and Opus 4/4.1. Search results can be provided from tool calls (dynamic RAG) or as top-level content (pre-fetched data). Each result requires source URL/identifier, title, and content array of text blocks, with optional citations configuration and cache_control. When citations are enabled, Claude automatically cites information with search_result_location citations containing source, title, cited_text, search_result_index, and block indices. The feature eliminates document-based workarounds and provides web search-quality citations for any content. Best practices include using clear permanent source URLs, providing descriptive titles, breaking long content into logical blocks, maintaining consistency, and handling errors gracefully. Search results can be combined with other content types and support cache control for performance optimization. Citations are all-or-nothing per request.

### **streaming.md**
Setting `"stream": true` enables incremental response streaming via server-sent events (SSE). The event flow includes message_start, content_block_start, content_block_delta events, content_block_stop, message_delta, and message_stop. SDKs provide convenient streaming interfaces for both sync and async operations. Content block deltas include text_delta (text content), input_json_delta (tool use parameters as partial JSON strings), and thinking_delta (for extended thinking). Important: current models emit one complete tool input key-value at a time, causing potential delays between events. Streaming examples demonstrate basic messages, tool use (with fine-grained parameter streaming), extended thinking (with signature_delta before block stop), and web search tool use (with server_tool_use and web_search_tool_result blocks). Error recovery involves capturing partial responses, constructing continuation requests with partial assistant responses, and resuming streaming. Best practices include using SDK features for message accumulation and being aware that tool use and thinking blocks cannot be partially recovered (resume from most recent text block).

### **text-generation.md**
Claude excels at text-based capabilities including: summarization (distilling content for executives, social media, product teams), content generation (blogs, emails, marketing, product descriptions), data/entity extraction (structured insights from unstructured text), question answering (chatbots, educational tutors), translation, analysis and recommendations (sentiment, preferences, personalization), dialogue and conversation (games, virtual assistants, storytelling), and code explanation/generation (reviews, boilerplate, tutorials). The guide links to practical cookbook examples for PDF upload and summarization, tool use and function calling, and embeddings with VoyageAI. Additional resources include the prompt engineering guide, prompt library for pre-crafted prompts, and comprehensive API documentation.

### **token-counting.md**
The token counting endpoint (`/v1/messages/count_tokens`) determines token counts before sending to Claude, helping manage rate limits, costs, and prompt optimization. Supported on all Claude 4, 3.7, 3.5, and 3 models, it accepts the same structured inputs as Messages API including system prompts, tools, images, PDFs, and extended thinking. Token counts are estimates and may differ slightly from actual usage. The endpoint is free but subject to RPM rate limits (100-8000 depending on usage tier). Examples demonstrate counting for basic messages, tools (server tool counts apply only to first sampling call), images, extended thinking (previous turn thinking blocks ignored, current turn counted), and PDFs. Important: Token counts may include system-added tokens for optimizations, but billing reflects only user content. With extended thinking, thinking blocks from previous turns don't count, but current turn thinking does. The endpoint supports context management to preview post-editing token usage.

### **tool-use.md**
Claude supports two tool types: client tools (user-defined and Anthropic-defined like computer use/text editor, executing on client systems) and server tools (web search/web fetch, executing on Anthropic servers). Client tool workflow involves providing tool definitions, Claude deciding to use tools (stop_reason: tool_use), executing tools and returning results, and Claude formulating responses. Server tools are executed automatically by Claude. Tool definitions use versioned types (e.g., web_search_20250305, text_editor_20250124). Claude can make parallel tool calls (multiple tool_use blocks in single response requiring all results returned together) or sequential calls (using output of one tool as input to another). Advanced patterns include chain of thought tool use (prompting Sonnet/Haiku to assess before calling tools), JSON mode (using tools to enforce structured output with tool_choice forcing), and missing information handling (Opus asks for missing params, Sonnet may infer). Pricing includes input tokens (tools parameter, tool_use blocks, tool_result blocks), output tokens, and server tool usage charges. Tool use system prompt token counts vary by model (264-530 tokens depending on tool_choice and model).

### **vision.md**
Claude's vision capabilities enable analyzing images via claude.ai, Console Workbench, or API. Supports up to 20 images on claude.ai and 100 via API (with 32MB request size limit), in JPEG, PNG, GIF, or WebP formats. Images can be provided as base64-encoded data, URL references, or via Files API file_id. For optimal performance, resize images to ≤1568 pixels on long edge and ≤1.15 megapixels to improve time-to-first-token. Token calculation: `(width * height)/750` for non-resized images. Cost examples at $3/MTok: 200x200px costs $0.16/1K images, 1092x1092px costs $4.80/1K images. Best practices include using supported formats, ensuring image clarity, and making text legible. Prompting examples demonstrate single images, multiple images (introduced as "Image 1:", "Image 2:"), multi-turn conversations with images, and combining with system prompts. Limitations include people identification refusal, potential inaccuracy with low-quality/rotated/small images, limited spatial reasoning, approximate counting, inability to detect AI-generated images, inappropriate content blocking, and healthcare scan interpretation limitations. Claude analyzes but cannot generate, produce, edit, or create images.

## Subdirectories

### **prompt-engineering/**
Comprehensive guides on prompt engineering techniques for optimizing Claude's performance across various tasks and use cases
- be-clear-and-direct.md: Golden rule of explicit, contextual instructions with examples showing transformation from vague to clear prompts
- chain-of-thought.md: Step-by-step reasoning techniques (basic, guided, structured with XML) for complex analysis and problem-solving
- chain-prompts.md: Breaking complex tasks into subtasks for accuracy and traceability with multi-step workflow examples
